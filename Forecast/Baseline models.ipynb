{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "862b62e0",
   "metadata": {},
   "source": [
    "# Baseline models\n",
    "Several models were designed as baseline: \n",
    "- Copy-Paste: Copy the latest data\n",
    "- Markovian: Use all of the data from the previous day\n",
    "- Non-Markovian: Use all the data up to the decorrelation time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07c23a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import useful libraries\n",
    "import importlib\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import signal\n",
    "from scipy import sparse\n",
    "stf_dir = os.path.join(os.pardir, 'spacetimeformer')\n",
    "sys.path.append(stf_dir)\n",
    "#from spacetimeformer import data\n",
    "#importlib.reload(data) #to make sure the last version of stf is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bba5aba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.forecaster at 0x280c5a97f70>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#delete if not used by Thursday\n",
    "class forecaster():\n",
    "    def __init__(self,model,dset,target,lag):\n",
    "        assert(model in ['CP','Mark','NMark','LSTM'])\n",
    "        assert(dset in ['full','selected'])\n",
    "        assert(target in ['chla','clusters','chcy'])\n",
    "                \n",
    "        \n",
    "    def fit(self,x_t,y_t):\n",
    "        return\n",
    "    def score(self,x_v,y_v):\n",
    "        return\n",
    "    \n",
    "            \n",
    "forecaster('CP','selected','chla',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2f42dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the dataset\n",
    "data_path = \"..\\\\Datasets\\\\Forecasting_aqua\\\\data_h_rolling_cst_int_v4.csv\"\n",
    "format = \"%Y-%m-%d %H:%M:%S\"\n",
    "df = pd.read_csv(data_path)\n",
    "time_df = pd.to_datetime(df[\"Datetime\"], format=format)\n",
    "try:\n",
    "    df.drop(\"sample\",axis=1,inplace=True)\n",
    "except:\n",
    "    pass\n",
    "df['Datetime']= time_df\n",
    "#According to the work of Stefanie Merkly, these keys were more important than the others to predict chorophill a\n",
    "keys_imp = ['ciliate','mean_chla','cv_chla_day','cv_chla_depth',\n",
    "            'nauplius','Ammonium','Nitrat','mean_schmidt','windspeed_max',\n",
    "            'mean_thermocline_depth','mean_epi_temp','mean_oxycline_depth',\n",
    "            'mean_mixed_layer_depth','mean_global_radiation'\n",
    "           ]\n",
    "#add some usefull variables at the end of the table\n",
    "df['year'] = df['Datetime'].dt.year\n",
    "df['month'] = df['Datetime'].dt.month\n",
    "df['day'] = df['Datetime'].dt.day\n",
    "\n",
    "#drop the first datapoints the first months are only meteorological data\n",
    "first_point = (~df['diatom'].isnull()).idxmax()\n",
    "dfs = df.drop(index=range(first_point))\n",
    "df = dfs.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e57ddf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(y_true,y_pred):\n",
    "    R2 = 1-np.sum(np.square(y_true-y_pred))/np.sum(np.square(y_true-np.mean(y_true,axis=0)))\n",
    "    MAEn = 1-np.sum(np.abs(y_true-y_pred))/np.sum(np.abs(y_true-np.mean(y_true,axis=0)))\n",
    "    return {'R2':R2,'MAEn':MAEn}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "898b52f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold(model,input_tv,n_fold,input_test,target_tv,target_test):\n",
    "    breakpoints = np.linspace(0,len(input_tv),n_fold+1,dtype=int)\n",
    "    res = []\n",
    "    for fold in range(n_fold):\n",
    "        print(f\"Fitting fold n°{fold}\")\n",
    "        input_v = input_tv[breakpoints[fold]:breakpoints[fold+1]]\n",
    "        y_v  = target_tv[breakpoints[fold]:breakpoints[fold+1]]\n",
    "        input_t = pd.concat([input_tv[:breakpoints[fold]],input_tv[breakpoints[fold+1]:]])\n",
    "        y_tr = pd.concat([target_tv[:breakpoints[fold]],target_tv[breakpoints[fold+1]:]])\n",
    "        model.fit(input_t,y_tr,[breakpoints[fold],breakpoints[fold+1]])\n",
    "        y_predtr = model.pred(input_t)\n",
    "        y_predv = model.pred(input_v)\n",
    "        y_predte = model.pred(input_test)\n",
    "        \n",
    "        s_train = score(y_tr.to_numpy(),y_predtr)\n",
    "        s_val = score(y_v.to_numpy(),y_predv)\n",
    "        s_test = score(target_test.to_numpy(),y_predte)\n",
    "        res.append({'fold':fold,'bp': breakpoints, 'y_predv':y_predv,'s_train':s_train,'s_val':s_val,'s_test':s_test})\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "193651f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CP_model:\n",
    "    def __init__(self,target_keys:list):\n",
    "        self.targets = target_keys\n",
    "    def fit(self,inputs,targets,breakpoints):\n",
    "        return\n",
    "    def pred(self,inputs):\n",
    "        return inputs[self.targets].to_numpy()\n",
    "class Markov_model:\n",
    "    def __init__(self,target_keys:list, inputs_keys:list,norm=True):\n",
    "        from sklearn.linear_model import LinearRegression\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        self.skmodel = LinearRegression()\n",
    "        if norm:\n",
    "            self._scalerin = StandardScaler()\n",
    "            self._scalerout = StandardScaler()\n",
    "    def fit(self,inputs,target,breakpoints):\n",
    "        inputs = inputs.drop(columns=['Datetime'],errors='ignore')\n",
    "        self._scalerin = self._scalerin.fit(inputs.values)\n",
    "        self._scalerout = self._scalerout.fit(target.values)\n",
    "        ninp = self._scalerin.transform(inputs.values)\n",
    "        #fill nans with 0s\n",
    "        ninp = np.nan_to_num(ninp)\n",
    "        ntar = self._scalerout.transform(target.values)\n",
    "        self.skmodel.fit(ninp,ntar)\n",
    "    def pred(self,inputs):\n",
    "        inputs = inputs.drop(columns=['Datetime'],errors='ignore')\n",
    "        ninp = self._scalerin.transform(inputs.values)\n",
    "        ninp = np.nan_to_num(ninp)\n",
    "        npred = self.skmodel.predict(ninp)\n",
    "        return self._scalerout.inverse_transform(npred)\n",
    "    \n",
    "\n",
    "class NONMarkov_model:\n",
    "    def __init__(self,target_keys:list, inputs_keys:list,norm=True,cor_th = 0.2,\n",
    "                 max_lag = 60*24,decimate = 10):\n",
    "        from sklearn.linear_model import LinearRegression\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        #get the decorrelation time\n",
    "        self.skmodel = [LinearRegression() for dim in range(len(target_keys))]\n",
    "        self.target_keys = target_keys\n",
    "        if norm:\n",
    "            self._scalerin = StandardScaler()\n",
    "            self._scalerout = StandardScaler()\n",
    "        self.cor_th = cor_th\n",
    "        self.max_lag = max_lag\n",
    "        self.decimate = decimate\n",
    "        self.cor_masks = []\n",
    "    def fit(self,inputs,targets,breakpoints):\n",
    "        inputs = inputs.drop(columns=['Datetime'],errors='ignore')\n",
    "        #fill the validation fold with 0:\n",
    "        self._scalerin = self._scalerin.fit(inputs.values)\n",
    "        self._scalerout = self._scalerout.fit(targets.values)\n",
    "        ninp = self._scalerin.transform(inputs.values)\n",
    "        #fill nans with 0s\n",
    "        ninp = np.nan_to_num(ninp)\n",
    "        ntar = self._scalerout.transform(targets.values)\n",
    "        ntar = np.nan_to_num(ntar)\n",
    "        #process each out key separatly:\n",
    "        ninp_pad = np.concatenate([ninp[:breakpoints[0]],\n",
    "                                   np.zeros((breakpoints[1]-breakpoints[0],ninp.shape[1])),\n",
    "                                   ninp[breakpoints[1]:]])\n",
    "        ntar_pad = np.concatenate([ntar[:breakpoints[0]],\n",
    "                                   np.zeros((breakpoints[1]-breakpoints[0],ntar.shape[1])),\n",
    "                                   ntar[breakpoints[1]:]])\n",
    "\n",
    "        for target in range(ntar.shape[1]):\n",
    "            cor = signal.correlate(ninp_pad,np.expand_dims(ntar_pad[:,target],-1), mode='full')/len(ntar_pad)\n",
    "            #note: due to the empty space left by the validation set, the correlation cannot reach one\n",
    "            cor_lag = signal.correlation_lags(ninp_pad.shape[0],ntar_pad.shape[0], mode='full')\n",
    "            idx0 = np.where(cor_lag==0)[0][0]\n",
    "            cor_past = cor[:idx0]\n",
    "            cor_mask = cor_past>self.cor_th\n",
    "            longest_lag = np.argmax(np.any(cor_mask,axis=1))\n",
    "            max_lag = min(longest_lag,self.max_lag)\n",
    "            #cor_lag = cor_lag[max_lag:idx0-24*lag]\n",
    "            cor_mask = cor_mask[idx0-max_lag:idx0]\n",
    "            self.cor_masks.append(cor_mask)\n",
    "\n",
    "            #format the input\n",
    "            start_inp = np.concatenate([np.arange(0,breakpoints[0]-max_lag),\n",
    "                                        np.arange(breakpoints[1],ninp.shape[0]-max_lag)])\n",
    "            inp_nonmarkov = np.zeros((len(start_inp)//self.decimate+1,max_lag*ninp.shape[1]))\n",
    "            tar_nonmarkov = np.zeros(len(start_inp)//self.decimate+1)\n",
    "            for idx,start in enumerate(start_inp[::self.decimate]):\n",
    "                inp_nonmarkov[idx,:] = (cor_mask * ninp[start:start+max_lag]).flatten()\n",
    "                tar_nonmarkov[idx] = ntar[start+max_lag]\n",
    "            #fit the model:\n",
    "            print(\"inputs formated\")\n",
    "            model = self.skmodel[target].fit(inp_nonmarkov,tar_nonmarkov)\n",
    "            print(\"model trained\")\n",
    "    def pred(self,inputs):\n",
    "        inputs = inputs.drop(columns=['Datetime'],errors='ignore')\n",
    "        ninp = self._scalerin.transform(inputs.values)\n",
    "        ninp = np.nan_to_num(ninp)\n",
    "        pred_nonmarkov = np.empty((len(inputs),len(self.target_keys)))\n",
    "        pred_nonmarkov[:] = np.nan\n",
    "                                  \n",
    "        #format:\n",
    "        for target in len(self.target_keys):\n",
    "            cor_mask = self.cor_masks[target]\n",
    "            #format the input\n",
    "            start_inp = np.arange(0,len(ninp)-max_lag)\n",
    "            inp_nonmarkov = np.zeros((len(start_inp),max_lag*ninp.shape[1]))\n",
    "            \n",
    "            for idx,start in enumerate(start_inp):\n",
    "                inp_nonmarkov[idx,:] = (cor_mask * ninp[start:start+max_lag,:]).flatten()\n",
    "            pred_nonmarkov[-len(start_inp):,target] = self.skmodel[target].predict(inp_nonmarkov)\n",
    "        return self._scalerout.inverse_transform(npred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea5930d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "af131c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#init:\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scalerin = StandardScaler()\n",
    "scalerout = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a8e554f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs formated\n",
      "model trained\n"
     ]
    }
   ],
   "source": [
    "i = 3000\n",
    "breakpoints = [i,i+2194]\n",
    "inputs = inputs_tv.set_index('Datetime')\n",
    "target = targets_tv['mean_chla_depth']\n",
    "scalerin = scalerin.fit(inputs.values)\n",
    "scalerout = scalerout.fit(target.values.reshape(-1, 1))\n",
    "ninp = scalerin.transform(inputs.values)\n",
    "ninp = np.nan_to_num(ninp)\n",
    "ntar = scalerout.transform(target.values.reshape(-1, 1))\n",
    "ntar = np.nan_to_num(ntar)\n",
    "ninp_pad = np.concatenate([ninp[:breakpoints[0]],np.zeros((breakpoints[1]-breakpoints[0],ninp.shape[1])),ninp[breakpoints[1]:]])\n",
    "ntar_pad = np.concatenate([ntar[:breakpoints[0]],np.zeros((breakpoints[1]-breakpoints[0],ntar.shape[1])),ntar[breakpoints[1]:]])\n",
    "\n",
    "cor = signal.correlate(ninp_pad,np.expand_dims(ntar_pad[:,0],-1), mode='full')/len(ntar_pad)\n",
    "#note: due to the empty space left by the validation set, the correlation cannot reach one\n",
    "\n",
    "cor_lag = signal.correlation_lags(ninp_pad.shape[0],ntar_pad.shape[0], mode='full')\n",
    "idx0 = np.where(cor_lag==0)[0][0]\n",
    "cor_past = cor[:idx0]\n",
    "cor_mask = cor_past>0.2\n",
    "longest_lag = np.argmax(np.any(cor_mask,axis=1))\n",
    "max_lag = min(longest_lag,24*50)\n",
    "#cor_lag = cor_lag[max_lag:idx0-24*lag]\n",
    "cor_mask = cor_mask[idx0-max_lag:idx0]\n",
    "\n",
    "#format the input\n",
    "decimate =5\n",
    "start_inp = np.concatenate([np.arange(0,breakpoints[0]-max_lag),np.arange(breakpoints[1],ninp.shape[0]-max_lag)])\n",
    "inp_nonmarkov = np.zeros((len(start_inp)//decimate+1,max_lag*ninp.shape[1]))\n",
    "tar_nonmarkov = np.zeros(len(start_inp)//decimate+1)\n",
    "for idx,start in enumerate(start_inp[::decimate]):\n",
    "    inp_nonmarkov[idx,:] = (cor_mask * ninp[start:start+max_lag]).flatten()\n",
    "    \n",
    "    tar_nonmarkov[idx] = ntar[start+max_lag]\n",
    "#fit the model:\n",
    "print(\"inputs formated\")\n",
    "model = LinearRegression().fit(inp_nonmarkov,tar_nonmarkov)\n",
    "print(\"model trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ffc3ba1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = CP_model(['mean_chla_depth'])\n",
    "model = NONMarkov_model(['mean_chla_depth','cluster_1'],df.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b51ccef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prop = 0.1\n",
    "n_fold = 10\n",
    "lag = 1\n",
    "in_keys = df.keys()\n",
    "out_keys = ['mean_chla_depth','cluster_1']\n",
    "test_len = int(len(df)*test_prop)\n",
    "dftest = df[-test_len:].reset_index(drop=True)\n",
    "dftv = df[:-test_len]\n",
    "\n",
    "inputs_tv = dftv[in_keys][:-24*lag]\n",
    "targets_tv = dftv[out_keys][24*lag:].reset_index(drop=True)\n",
    "\n",
    "inputs_test = dftest[in_keys][:-24*lag]\n",
    "targets_test = dftest[out_keys][24*lag:].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d446bf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = kfold(model,inputs_tv,n_fold,inputs_test,targets_tv,targets_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2ef4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting fold n°10\n"
     ]
    }
   ],
   "source": [
    "res = kfold(model,inputs_tv,n_fold,inputs_test,targets_tv,targets_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b256ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nice_plot(inputs_tv,inputs_test,targets_tv,targets_test,res):\n",
    "    fig,axs = plt.subplots(len(targets_tv.keys()),figsize = (15,6*len(targets_tv.keys())))\n",
    "    if len(targets_tv.keys()):\n",
    "        axs=[axs]\n",
    "    for idx,(ax,key) in enumerate(zip(axs,targets_tv.keys())):\n",
    "        val_concat = np.concatenate([f['y_predv'] for f in res])\n",
    "        true = ax.scatter(inputs_tv['Datetime'],targets_tv.to_numpy(),s=1)\n",
    "        for fold in range(len(res[0]['bp'])-1):\n",
    "            pred = ax.scatter(inputs_tv['Datetime'][res[0]['bp'][fold]:res[0]['bp'][fold+1]],res[fold]['y_predv'],s=0.5)\n",
    "def plot_res(res):\n",
    "    print(f\"R2 val: {np.mean([f['s_val']['R2'] for f in res]):.2} +/- {2*np.std([f['s_val']['R2'] for f in res]):.2}\")\n",
    "    print(f\"R2 test: {np.mean([f['s_test']['R2'] for f in res]):.2} +/- {2*np.std([f['s_val']['R2'] for f in res]):.2}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dac45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nice_plot(inputs_tv,inputs_test,targets_tv,targets_test,res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87141bed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_res(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c343f59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
